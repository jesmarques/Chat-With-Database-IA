import os
import streamlit as st
from dotenv import load_dotenv
from langchain_community.utilities import SQLDatabase
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_groq import ChatGroq
from langchain_community.agent_toolkits import create_sql_agent

# 1. Seguran√ßa de Credenciais: Carregar vari√°veis do .env
load_dotenv()
#api_key = os.getenv("GOOGLE_API_KEY")
api_key = os.getenv("GROQ_API_KEY")

st.set_page_config(page_title="E-commerce Data Agent", page_icon="ü§ñ", layout="centered")
st.title("ü§ñ Assistente de Dados - Olist")
st.markdown("Fa√ßa perguntas em portugu√™s sobre as vendas, clientes e produtos.")

# Verifica se a chave foi carregada corretamente
if not api_key:
    st.error("Chave da API n√£o encontrada. Verifique se o arquivo .env est√° configurado corretamente na raiz do projeto.")
    st.stop()

@st.cache_resource
def configurar_agente():
    # 2. Seguran√ßa de Infraestrutura: Conectando ao SQLite em modo SOMENTE LEITURA (Read-Only)
    # O par√¢metro ?mode=ro&uri=true √© a trava f√≠sica. O banco rejeitar√° qualquer INSERT/DELETE.
    db = SQLDatabase.from_uri("sqlite:///file:ecommerce.db?mode=ro&uri=true")
    
    # Inicializa o modelo da IA
    #llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=api_key, temperature=0)
    #llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", google_api_key=api_key, temperature=0)
    #llm = ChatGoogleGenerativeAI(model="gemini-flash-latest", google_api_key=api_key, temperature=0)
    #llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash-lite-001", google_api_key=api_key, temperature=0)
    # Usando o modelo que est√° ativo no seu terminal (Llama 3.3 70B Versatile)
    llm = ChatGroq(
        model="llama-3.3-70b-versatile", 
        api_key=api_key, 
        temperature=0
    )
    
    # 3. Seguran√ßa Cognitiva: Instru√ß√µes expl√≠citas (System Prompt)
    instrucoes_seguranca = """
    Voc√™ √© um analista de dados s√™nior focado em responder perguntas de neg√≥cios.
    Voc√™ tem acesso a um banco de dados SQLite.
    
    SUAS REGRAS DE OURO:
    1. Voc√™ S√ì tem permiss√£o para executar consultas SELECT.
    2. NUNCA tente executar comandos como INSERT, UPDATE, DELETE, DROP ou ALTER.
    3. Se o usu√°rio pedir para apagar ou alterar algum dado, recuse educadamente explicando que voc√™ √© um agente apenas de leitura focado em an√°lise de dados.
    4. ATEN√á√ÉO!! NUNCA inclua seus pensamentos internos na resposta final (como "Finally, I should respond..."). Responda DIRETAMENTE ao usu√°rio e APENAS em portugu√™s.
    5. Se pedirem para apagar algo, apenas responda que n√£o tem permiss√£o. N√ÉO tente listar os dados que seriam apagados, a menos que o usu√°rio pe√ßa explicitamente.
    6. Sempre limite suas consultas SQL a no m√°ximo 10 linhas (LIMIT 10) para evitar sobrecarga, a menos que pe√ßam para contar (COUNT).
    """
    
    # Cria o Agente com a trava cognitiva injetada (prefix)
    agente = create_sql_agent(
        llm, 
        db=db, 
        agent_type="zero-shot-react-description", # A M√ÅGICA ACONTECE AQUI
        verbose=True,
        prefix=instrucoes_seguranca,
        handle_parsing_errors=True # Colete √† prova de balas contra erros de formata√ß√£o
    )
    return agente

agente_sql = configurar_agente()

# --- L√ìGICA DO CHAT (Interface) ---
if "mensagens" not in st.session_state:
    
    st.session_state.mensagens = []

# Mostra o hist√≥rico
for msg in st.session_state.mensagens:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Caixa de input do usu√°rio
pergunta = st.chat_input("Pergunte algo ao banco de dados (ex: Qual o estado com mais clientes?)...")

if pergunta:
    # Registra a pergunta do usu√°rio
    st.session_state.mensagens.append({"role": "user", "content": pergunta})
    with st.chat_message("user"):
        st.markdown(pergunta)

    # IA processando
    with st.chat_message("assistant"):
        with st.spinner("Analisando o banco de dados e gerando a consulta SQL..."):
            try:
                import ast

                # O LangChain toma o controle aqui
                resposta = agente_sql.invoke({"input": pergunta})
                resultado_final = resposta["output"]
                
                # --- NOVO FILTRO DE LIMPEZA ROBUSTO ---
                # 1. Se vier como texto parecendo uma lista, converte para lista real
                if isinstance(resultado_final, str) and resultado_final.strip().startswith("["):
                    try:
                        resultado_final = ast.literal_eval(resultado_final)
                    except:
                        pass
                
                # 2. Se for uma lista (pois o Gemini √†s vezes picota a resposta)
                if isinstance(resultado_final, list):
                    texto_construido = ""
                    for pedaco in resultado_final:
                        if isinstance(pedaco, str):
                            texto_construido += pedaco # Se for texto puro, junta
                        elif isinstance(pedaco, dict) and "text" in pedaco:
                            texto_construido += pedaco["text"] # Se for dicion√°rio, extrai o texto e junta
                    resultado_final = texto_construido
                # ---------------------------------------

                # --- NOVO: LIMPANDO O VAZAMENTO COM REGEX (MODO DEFINITIVO) ---
                import re
                # Ca√ßa qualquer frase que comece com "Finally," e termine no primeiro ponto final, e a destr√≥i.
                resultado_final = re.sub(r'(?i)Finally,.*?\.', '', resultado_final).strip()
                # --------------------------------------------------------------

                st.markdown(resultado_final)
                
                # Registra a resposta da IA limpa
                st.session_state.mensagens.append({"role": "assistant", "content": resultado_final})
            
            except Exception as e:
                mensagem_erro = str(e)
                
                if "Could not parse LLM output:" in mensagem_erro:
                    # 1. Extrai a resposta real
                    resposta_escondida = mensagem_erro.split("Could not parse LLM output:")[1].strip()
                    
                    # 2. LIMPEZA TOTAL: Remove o link, as crases e a frase de erro do LangChain
                    import re
                    # Remove a frase "For troubleshooting, visit:" e qualquer link que venha depois
                    resposta_escondida = re.sub(r'For troubleshooting, visit:.*', '', resposta_escondida)
                    # Remove links soltos que ainda possam existir
                    resposta_escondida = re.sub(r'https?://\S+', '', resposta_escondida)
                    # Remove crases e espa√ßos em branco nas pontas
                    resposta_escondida = resposta_escondida.replace("`", "").strip()
                    
                    st.markdown(resposta_escondida)
                    st.session_state.mensagens.append({"role": "assistant", "content": resposta_escondida})
                else:
                    st.error(f"Erro na execu√ß√£o da consulta. Detalhes: {e}")